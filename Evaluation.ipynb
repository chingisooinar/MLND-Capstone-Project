{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "from PIL import Image\n",
    "from torch_snippets import *\n",
    "from torchvision.utils import make_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "val_set = glob('maps/val/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Val set size: {val_set.__len__()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "both_transform = A.Compose(\n",
    "    [A.Resize(width=256, height=256)], additional_targets={\"image0\": \"image\"},\n",
    ")\n",
    "transform_only_input = A.Compose(\n",
    "    [\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_only_mask = A.Compose(\n",
    "    [\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "class MapDataset(Dataset):\n",
    "    def __init__(self, images):\n",
    "\n",
    "        self.list_files = images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.list_files[index]\n",
    "        image = np.array(Image.open(img_path))\n",
    "        input_image = image[:, :600, :]\n",
    "        target_image = image[:, 600:, :]\n",
    "\n",
    "        augmentations = both_transform(image=input_image, image0=target_image)\n",
    "        input_image = augmentations[\"image\"]\n",
    "        target_image = augmentations[\"image0\"]\n",
    "\n",
    "        input_image = transform_only_input(image=input_image)[\"image\"]\n",
    "        target_image = transform_only_mask(image=target_image)[\"image\"]\n",
    "\n",
    "        return input_image, target_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = MapDataset(val_set)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for i_ in range(16):\n",
    "  samples_input, samples_target = next(iter(val_loader))\n",
    "  samples = torch.cat([samples_input, samples_target],axis=0)\n",
    "  images.append(samples)\n",
    "images = torch.cat(images,axis=0)\n",
    "images = make_grid((images * 127.5) + 127.5).permute(1,2,0)\n",
    "show(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.load('BCE+L1+StyleLoss/generator.pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrimintor_bce = torch.load('BCE/discriminator.pt').to(device)\n",
    "discrimintor_bce_l1 = torch.load('BCE + L1/discriminator.pt').to(device)\n",
    "discrimintor_bce_l1_st = torch.load('BCE+L1+StyleLoss/discriminator.pt').to(device)\n",
    "discrimintor_l2_l1 = torch.load('L2+L1/discriminator.pt').to(device)\n",
    "discriminators = [discrimintor_bce, discrimintor_bce_l1, discrimintor_bce_l1_st, discrimintor_l2_l1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BCE = nn.BCEWithLogitsLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_step(real_src, fake_trg, discriminator):\n",
    "    #discriminator.train()\n",
    "    prediction_fake = discriminator(real_src, fake_trg.detach())\n",
    "    error_fake = BCE(prediction_fake, torch.zeros_like(prediction_fake))\n",
    "    D_loss = error_fake\n",
    "\n",
    "    return D_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "log = Report(epochs)\n",
    "\n",
    "N = len(val_loader)\n",
    "generator.eval()\n",
    "for discriminator in discriminators:\n",
    "    discriminator.eval()\n",
    "with torch.no_grad():\n",
    "    images = []\n",
    "    errors = []\n",
    "    for bx, batch in enumerate(val_loader):\n",
    "        real_src, real_trg = batch\n",
    "        real_src, real_trg = real_src.to(device), real_trg.to(device)\n",
    "        fake_trg = generator(real_src)\n",
    "        \n",
    "        samples = torch.cat([real_src, fake_trg],axis=0)\n",
    "        images.append(samples)\n",
    "        \n",
    "        errD=0\n",
    "        for discriminator in discriminators:\n",
    "            errD += discriminator_step(real_src, fake_trg, discriminator)\n",
    "        \n",
    "        log.record(pos=epochs+(1+bx)/N, errD=errD.item()/len(discriminators), end='\\r')\n",
    "        errors.append(errD.item()/len(discriminators))\n",
    "        if len(images) == 16:\n",
    "            images = torch.cat(images,axis=0)\n",
    "            images = make_grid((images * 127.5) + 127.5).permute(1,2,0)\n",
    "            show(images)\n",
    "            images = list()\n",
    "print(f'Average loss: {sum(errors)/len(errors)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
